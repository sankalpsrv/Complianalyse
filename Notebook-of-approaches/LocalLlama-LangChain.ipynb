{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164368ef-e0e4-4951-8276-1ca2e6182404",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Due to the limitations of AzureMLEndpoints for reusability on local environments (people without keys cannot access), it was considered feasible to try out Local Deployment of Llama2 to see whether the full modification of the code (by adding, not replacing) should be initiated. \n",
    "\n",
    "### Notebook contents\n",
    "The notebook tests the capabilities of Llama2's 7B model for generation of XML in the LegalRuleML format according to the GPTQ quantised model available on HuggingFace and incorporated via LangChain.\n",
    "\n",
    "### Results\n",
    "There has been a limitation with respect to context-windows in other quantisation methods, which has apparently been bypassed with subtlety in the GPTQ quantised model. It has generated the XML fairly accurately in accordance with a single example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e953685-1218-419e-89ef-da4fa41b4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import requests\n",
    "import os\n",
    "import urllib.request\n",
    "import ssl\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0cd07ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/Schematise-dev-server/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a6d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba2c0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Statute: Punishment for piracy - Whoever commits any act of piracy shall be punished with imprisonment for life or with fine or with both; or with death if the act of piracy causes death.\n",
      "AI: \n",
      "<LegalRuleML xmlns=\"http://www.oasis-open.org/committees/legalruleml\">\n",
      "  <lrml:PrescriptiveStatement id=\"PunishmentForPiracy\">\n",
      "    <lrml:Rule id=\"RuleForPiracyPunishment\">\n",
      "      <lrml:if>\n",
      "      <lrml:Fact>\n",
      "          <lrml:Rel iri=\"#commitPiracy\"/>\n",
      "        </lrml:Fact>\n",
      "      </lrml:if>\n",
      "      <lrml:then>\n",
      "        <lrml:Obligation>\n",
      "          <lrml:Sanction>\n",
      "            <lrml:Penalty>\n",
      "              <lrml:Type iri=\"#imprisonment\"/>\n",
      "              <lrml:Duration max=\"life\"/>\n",
      "            </lrml:Penalty>\n",
      "            <lrml:Condition>\n",
      "              <lrml:CausesDeathOrAttempt/>\n",
      "              <lrml:AdditionalSanctions>\n",
      "                <lrml:Restitution/>\n",
      "                <lrml:ForfeitureOfProperty/>\n",
      "              </lrml:AdditionalSanctions>\n",
      "            </lrml:Condition>\n",
      "          </lrml:Sanction>\n",
      "        </lrml:Obligation>\n",
      "      </lrml:then>\n",
      "    </lrml:Rule>\n",
      "  </lrml:PrescriptiveStatement>\n",
      "</LegalRuleML>\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Statute: Punishment for piracy - Whoever commits any act of piracy shall be punished with imprisonment for life or with fine or with both; or with death if the act of piracy causes death.\",\n",
    "        \"answer\": \"\"\"\n",
    "<LegalRuleML xmlns=\"http://www.oasis-open.org/committees/legalruleml\">\n",
    "  <lrml:PrescriptiveStatement id=\"PunishmentForPiracy\">\n",
    "    <lrml:Rule id=\"RuleForPiracyPunishment\">\n",
    "      <lrml:if>\n",
    "      <lrml:Fact>\n",
    "          <lrml:Rel iri=\"#commitPiracy\"/>\n",
    "        </lrml:Fact>\n",
    "      </lrml:if>\n",
    "      <lrml:then>\n",
    "        <lrml:Obligation>\n",
    "          <lrml:Sanction>\n",
    "            <lrml:Penalty>\n",
    "              <lrml:Type iri=\"#imprisonment\"/>\n",
    "              <lrml:Duration max=\"life\"/>\n",
    "            </lrml:Penalty>\n",
    "            <lrml:Condition>\n",
    "              <lrml:CausesDeathOrAttempt/>\n",
    "              <lrml:AdditionalSanctions>\n",
    "                <lrml:Restitution/>\n",
    "                <lrml:ForfeitureOfProperty/>\n",
    "              </lrml:AdditionalSanctions>\n",
    "            </lrml:Condition>\n",
    "          </lrml:Sanction>\n",
    "        </lrml:Obligation>\n",
    "      </lrml:then>\n",
    "    </lrml:Rule>\n",
    "  </lrml:PrescriptiveStatement>\n",
    "</LegalRuleML>\"\"\"}]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{question}\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d1b9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a consistent Legal Rule ML converter who takes the statute text as input and outputs the XML format for the same with reference to the documentation available at http://docs.oasis-open.org/legalruleml/legalruleml-core-spec/v1.0/legalruleml-core-spec-v1.0.html\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e042e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n6. Responsibilities of collection centres.\\tcollect e-waste on behalf of producer or dismantler or recycler or refurbisher including those arising from orphaned products;, Provided the collection centres established by producer can also collect e-waste on behalf of dismantler, refurbisher and recycler including those arising from orphaned products, ensure that the facilities are in accordance with the standards or guidelines issued by Central Pollution Control Board from time to time;, ensure that the e-waste collected by them is stored in a secured manner till it is sent to authorised dismantler or recycler as the case may be;, ensure that no damage is caused to the environment during storage and transportation of e-waste;, maintain records in Form-2 of the e-waste handled as per the guidelines of Central Pollution Control Board and make such records available for scrutiny by the Central Pollution Control Board or the concerned State Pollution Control Board as and when asked for.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = final_prompt | hf\n",
    "section_for_conversion = \"\"\"\n",
    "6. Responsibilities of collection centres.\tcollect e-waste on behalf of producer or dismantler or recycler or refurbisher including those arising from orphaned products;, Provided the collection centres established by producer can also collect e-waste on behalf of dismantler, refurbisher and recycler including those arising from orphaned products, ensure that the facilities are in accordance with the standards or guidelines issued by Central Pollution Control Board from time to time;, ensure that the e-waste collected by them is stored in a secured manner till it is sent to authorised dismantler or recycler as the case may be;, ensure that no damage is caused to the environment during storage and transportation of e-waste;, maintain records in Form-2 of the e-waste handled as per the guidelines of Central Pollution Control Board and make such records available for scrutiny by the Central Pollution Control Board or the concerned State Pollution Control Board as and when asked for.\n",
    "\"\"\"\n",
    "\n",
    "section_for_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "919ed805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "<LegalRuleML xmlns=\"http://www.oasis-open.org/committees/legalruleml\">\n",
      "  <lrml:PrescriptiveStatement id=\"ResponsibilitiesOfCollectionCentres\">\n",
      "    <lrml:Rule id=\"RuleForCollectingEWaste\">\n",
      "      <lrml:if>\n",
      "      <lrml:Fact>\n",
      "          <lrml:Rel iri=\"#collectEWasteOnBehalfProducerDismantlerRecyclerRefurbisherIncludingOrphanedProducts\"/>\n",
      "        </lrml:Fact>\n",
      "      </lrml:if>\n",
      "      <lrml:then>\n",
      "        <lrml:Obligation>\n",
      "          <lrml:Directive>\n",
      "            <lrml:Action>\n",
      "              <lrml:EstablishFacilitiesInAccordanceWithStandardsIssuedByCentralPollutionControlBoardFromTimeToTime/>\n",
      "            </lrml:Action>\n",
      "          </lrml:Directive>\n",
      "        </lrml:Obligation>\n",
      "      </lrml:then>\n",
      "    </lrml:Rule>\n",
      "  </lrml:PrescriptiveStatement>\n",
      "</LegalRuleML>\n"
     ]
    }
   ],
   "source": [
    "print (llm_chain.invoke({\"question\": section_for_conversion}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2181d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Schematise-dev-server",
   "language": "python",
   "name": "schematise-dev-server"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
